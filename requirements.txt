# we don't have a flash attention binary yet for Pytorch 2.7
# so we use pytorch 2.6
torch==2.6.0
torchvision==0.21.0
torchdata==0.10.1
torchao==0.9.0

# for video decoding
# Note: We need to install torchcodec with CUDA support for A100
# This will be handled in the Dockerfile with the correct index-url
torchcodec

# something broke in Transformers > 4.55.4
transformers==4.55.4

# For GPU monitoring of NVIDIA chipsets
pynvml

# we are waiting for the next PyPI release
#finetrainers==0.1.0
finetrainers @ git+https://github.com/huggingface/finetrainers.git@main
# temporary fix for pip install bug:
#finetrainers @ git+https://github.com/jbilcke-hf/finetrainers-patches.git@fix_missing_sft_trainer_files

# it is recommended to always use the latest version
diffusers @ git+https://github.com/huggingface/diffusers.git@main

imageio
imageio-ffmpeg

flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

# for youtube video download
pytube
pytubefix

# for scene splitting
scenedetect[opencv]

# for llava video / captionning
pillow
pillow-avif-plugin
polars
einops
open_clip_torch
av==14.1.0

# for some reason LLaVA-NeXT has ceased to work,
# but I think it it due to a breaking change in Transformers
git+https://github.com/LLaVA-VL/LLaVA-NeXT.git

# for our frontend
gradio==5.33.1
gradio_toggle
gradio_modal

# used for the monitor
matplotlib